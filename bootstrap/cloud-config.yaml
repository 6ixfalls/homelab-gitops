#cloud-config

# debug: true
users:
  - name: six
    shell: /bin/bash
    groups: [admin]
    ssh_authorized_keys: [github:6ixfalls]

strict: true

install:
  device: /dev/sda
  auto: true
  reboot: true
  extra-dirs-rootfs: &longhorn [/var/lib/longhorn]

upgrade:
  extra-dirs-rootfs: *longhorn
reset:
  extra-dirs-rootfs: *longhorn

growpart:
  devices: ["/"]

stages:
  after-install-chroot:
    - name: Format Longhorn disk
      if: "[ -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn ] && [ ! -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn-part1 ]"
      commands:
        - parted /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn --script -a optimal -- mklabel gpt mkpart longhorn ext4 0% 100%
    # -- (only `commands`,`entities` and `files` may have templating)
    - name: "Add control plane or worker config determined by mac address"
      files:
        - path: /oem/60_k3s_config.yaml
          permissions: 0666
          content: |
            #cloud-config
            {{- $control := false -}}
            {{- $first := false -}}
            {{- $macs := list "bc:24:11:ca:7b:77" "bc:24:11:fa:f2:eb" "bc:24:11:97:43:81" -}}
            {{- range $net := .Values.network -}}
              {{- if has $net.macaddress $macs -}}
                {{- $control = true -}}
              {{- end -}}
              {{- if eq $net.macaddress (first $macs) -}}
                {{- $first = true -}}
              {{ end -}}
            {{- end -}}
            {{- if $control }}
            k3s:
              enabled: true
              env:
                K3S_TOKEN: "{{ P2P_NETWORK_TOKEN }}"
              args:
            {{- if $first }}
                - --cluster-init
            {{- else }}
                - --server https://10.17.4.111:6443
            {{- end }}
                - --tls-san "10.17.4.111"
                - --disable traefik,servicelb,local-storage
                - --flannel-backend none
                - --disable-network-policy
                - --write-kubeconfig-mode 0644
                - --node-taint node-role.kubernetes.io/control-plane:NoSchedule
                - --disable-kube-proxy
            {{- else }}
            k3s-agent:
              enabled: true
              env:
                K3S_TOKEN: "{{ P2P_NETWORK_TOKEN }}"
              args:
                - --server https://10.17.4.111:6443
                - --node-label "node.longhorn.io/create-default-disk=true"
            {{- end }}
            {{- if $first }}
            stages:
              reconcile:
                - name: "Drop external secret"
                  if: "[ ! -e /var/lib/rancher/k3s/server/manifests/cluster-secrets.yaml ] && [ -e /etc/rancher/k3s/k3s.yaml ]"
                  files:
                    - path: /var/lib/rancher/k3s/server/manifests/cluster-secrets.yaml
                      content: |
                        apiVersion: v1
                        data:
                          environment: {{ INFISICAL_ENVIRONMENT }}
                          clientId: {{ INFISICAL_ID }}
                          clientSecret: {{ INFISICAL_SECRET }}
                          projectSlug: {{ INFISICAL_PROJECT }}
                          clusterDomain: {{ CLUSTER_DOMAIN }}
                        kind: Secret
                        metadata:
                          name: cluster-secrets
                          namespace: flux-system
                - name: "Add cluster-specific manifests: Flux"
                  if: "[ ! -e /var/lib/rancher/k3s/server/manifests/flux.applied ] && [ -e /etc/rancher/k3s/k3s.yaml ]"
                  files:
                    - path: /tmp/flux-manifest.sh
                      content: |
                        #!/bin/bash
                        set -x
                        kubectl apply --server-side --kustomize github.com/6ixfalls/homelab-gitops.git//bootstrap
                        kubectl apply --server-side --kustomize github.com/6ixfalls/homelab-gitops.git//clusters/hutao/flux/config
                        touch /var/lib/rancher/k3s/server/manifests/flux.applied
                      permissions: 0777
                  commands:
                    - "KUBECONFIG=/etc/rancher/k3s/k3s.yaml nohup /bin/bash /tmp/flux-manifest.sh > /tmp/flux-apply.out 2>&1 &"
                - name: "Add cluster-specific manifests: Cilium"
                  if: "[ ! -e /var/lib/rancher/k3s/server/manifests/cilium.applied ] && [ -e /etc/rancher/k3s/k3s.yaml ]"
                  downloads:
                    - url: https://raw.githubusercontent.com/6ixfalls/homelab-gitops/main/bootstrap/cilium.yaml
                      path: /tmp/cilium-manifest.yaml
                  files:
                    - path: /tmp/cilium-manifest.sh
                      content: |
                        #!/bin/bash
                        set -x
                        kubectl apply --server-side /tmp/cilium-manifest.yaml
                        touch /var/lib/rancher/k3s/server/manifests/cilium.applied
                      permissions: 0777
                  commands:
                    - "KUBECONFIG=/etc/rancher/k3s/k3s.yaml nohup /bin/bash /tmp/cilium-manifest.sh > /tmp/cilium-apply.out 2>&1 &"
                - name: "Add cluster-specific manifests: kube-vip"
                  if: "[ ! -e /var/lib/rancher/k3s/server/manifests/kube-vip.applied ] && [ -e /etc/rancher/k3s/k3s.yaml ]"
                  downloads:
                    - url: https://raw.githubusercontent.com/6ixfalls/homelab-gitops/main/bootstrap/kube-vip.yaml
                      path: /tmp/kube-vip-manifest.yaml
                  files:
                    - path: /tmp/kube-vip-manifest.sh
                      content: |
                        #!/bin/bash
                        set -x
                        kubectl apply --server-side /tmp/kube-vip-manifest.yaml
                        touch /var/lib/rancher/k3s/server/manifests/kube-vip.applied
                      permissions: 0777
                  commands:
                    - "KUBECONFIG=/etc/rancher/k3s/k3s.yaml nohup /bin/bash /tmp/kube-vip-manifest.sh > /tmp/kube-vip-apply.out 2>&1 &"
            {{- end }}
  # -- https://github.com/derailed/k9s/issues/1399
  initramfs:
    - name: Increase number of open files
      sysctl:
        fs.inotify.max_user_instances: "8192"
        fs.inotify.max_user_watches: "524288"
  boot:
    - name: "Format longhorn filesystem if unformatted"
      if: "[ -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn ] && [ -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn-part1 ] && [[ ! $(fsck -n dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn-part1) ]]"
      commands:
        - mkfs.ext4 -F /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn-part1
    - name: "Mount longhorn filesystem"
      if: "[ -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn ] && [ -e /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn-part1 ]"
      commands:
        # -- https://github.com/kairos-io/packages/blob/3482fed51f21a2155b60a0aa9ac91b1d839d1029/packages/static/kairos-overlay-files/files/system/oem/00_rootfs_uki.yaml#L43
        - umount /var/lib/longhorn
        - mount -o rw /dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_longhorn-part1 /var/lib/longhorn
    - name: "Set up various kube environment variables"
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
        CONTAINERD_ADDRESS: /run/k3s/containerd/containerd.sock
        CONTAINERD_NAMESPACE: k8s.io
    - name: "Drop initialization lock script"
      files:
        - path: /tmp/get-manifest-lock.sh
          content: |
            #!/bin/bash
            # Try for 30 minutes, with 15 second intervals
            minutes=30
            sleep=15
            retry_attempt=1
            total_attempts=$(( minutes * 60 / sleep ))
            active="false"

            while [[ $retry_attempt -le $total_attempts ]]; do
              if [[ "$active" != "true" ]]; then
                # Ensure only one host tries to bootstrap, whichever makes the configmap first
                if ! timeout 5 kubectl version &> /dev/null; then
                  echo "Kubernetes API not ready yet, sleeping"
                else
                  if ! timeout 5 kubectl create configmap $1 --from-literal=hostname="$(hostname)"; then
                    echo "Unable to create configmap, another node may be active"
                  fi

                  # The configmap exists but we must finally check if the hostname matches
                  if [[ "$(timeout 5 kubectl get configmap -n default $1 -o jsonpath='{.data.hostname}')" != "$(hostname)" ]]; then
                    echo "Bootstrap ConfigMap exists but another node is active, exiting..."
                    exit 3
                  fi

                  # We must be the active node
                  active="true"
                fi
              fi
                
              if [[ "$active" == "true" ]]; then
                exit 0
              fi

              echo "Install attempt $retry_attempt (of $total_attempts) failed, retrying in $sleep seconds"
              (( retry_attempt = retry_attempt + 1 ))
              sleep $sleep
            done

            # Failed all attempts
            exit 2
          permissions: 0777
